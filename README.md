# Entity Resolution with DSPy: Abt-Buy Product Matching

## Overview
This project demonstrates entity resolution (record linkage) for product data using [DSPy](https://github.com/stanfordnlp/dspy) and large language models (LLMs). It matches product records from two e-commerce sources (Abt and Buy.com) and predicts whether they refer to the same real-world product.

## Dataset
- **Source:** [Abt-Buy Entity Resolution Benchmark](https://dbs.uni-leipzig.de/research/projects/benchmark-datasets-for-entity-resolution#:~:text=Used%20in-,Abt%2DBuy,-E%2Dcommerce) ([Leipzig Database Group](https://dbs.uni-leipzig.de/))
- **Files:**
  - `Abt.csv`, `Buy.csv`: Product catalogs from two online shops
  - `abt_buy_perfectMapping.csv`: Ground-truth matching pairs
- **How to Download:**
  1. Visit the [Leipzig ER Benchmark page](https://dbs.uni-leipzig.de/research/projects/benchmark-datasets-for-entity-resolution#:~:text=Used%20in-,Abt%2DBuy,-E%2Dcommerce)
  2. Download the "Abt-Buy" dataset zip and extract the three CSVs into `data/raw/`

## Project Structure
```
.
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                # Original Abt-Buy data
‚îÇ   ‚îú‚îÄ‚îÄ train_products.csv  # Training pairs (generated)
‚îÇ   ‚îú‚îÄ‚îÄ val_products.csv    # Validation pairs (generated)
‚îÇ   ‚îî‚îÄ‚îÄ validation_results_bootstrapfewshot_2025-06-26.md  # Example results
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ data.py             # Data loading and formatting
‚îÇ   ‚îú‚îÄ‚îÄ module.py           # DSPy module definition
‚îÇ   ‚îî‚îÄ‚îÄ signature.py        # DSPy signature for product matching
‚îú‚îÄ‚îÄ create_train_val_split.py # Script to generate train/val splits
‚îú‚îÄ‚îÄ main.py                 # Main experiment script
‚îú‚îÄ‚îÄ requirements.txt        # Python dependencies
‚îî‚îÄ‚îÄ README.md               # This file
```

## Setup
1. **Install Python 3.9+** and [pip](https://pip.pypa.io/en/stable/).
2. **Create a virtual environment (recommended):**
   ```bash
   python -m venv .venv
   source .venv/bin/activate
   ```
3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```
4. **Set up your API keys:**
   - Create a `.env` file in the project root with the appropriate API key(s):
     ```
     # For OpenAI models
     OPENAI_API_KEY=your_openai_api_key_here
     
     # For OpenRouter models (optional)
     OPENROUTER_API_KEY=your_openrouter_api_key_here
     ```

## Data Preparation
1. **Download and extract the Abt-Buy dataset** as described above.
2. **Generate train/validation splits:**
   ```bash
   python create_train_val_split.py
   ```
   This will create `data/train_products.csv` and `data/val_products.csv`.

## Running the Experiment
Run the main script to evaluate entity resolution using DSPy and your chosen language model:
```bash
python main.py
```
- The script will:
  - Compile a DSPy program using few-shot learning (BootstrapFewShot)
  - Evaluate on a sample of the validation set
  - Print confusion matrix, classification report, and disagreement details

### Testing Model Providers
You can test each model provider individually:

```bash
# Test OpenRouter integration
python test_openrouter.py
```

## Model Configuration
The project supports three different model providers:

### 1. **Ollama (Local Models)**
- **Default setting**: Uses local models via Ollama
- **Requirements**: Ollama must be running locally
- **Configuration**: Edit `MODEL_PROVIDER = "ollama"` in `main.py`
- **Available models**: Any model installed in Ollama (e.g., `qwen3:14b`, `gemma:2b`, `llama3.2`)

### 2. **OpenAI**
- **Requirements**: OpenAI API key in `.env` file
- **Configuration**: Set `MODEL_PROVIDER = "openai"` in `main.py`
- **Available models**: GPT-4, GPT-3.5, etc.

### 3. **OpenRouter**
- **Requirements**: OpenRouter API key in `.env` file
- **Configuration**: Set `MODEL_PROVIDER = "openrouter"` in `main.py`
- **Available models**: Various models including Google Gemini, Anthropic Claude, etc.

## Customization
- **Model Provider:** Change `MODEL_PROVIDER` in `main.py` to switch between providers
- **Model Selection:** Edit the respective model name constants in `main.py`
- **Prompt/Model:** Edit `main.py` to change the system prompt, model, or number of few-shot demos
- **Validation Size:** Adjust the sample size or use the full validation set as needed
- **Teleprompters:** Try other DSPy teleprompters (see `main.py` for examples)

## Advanced Teleprompter Optimization

BootstrapFewShot is a great starting point. For potentially better performance, you can explore `dspy.teleprompt.BootstrapFewShotWithRandomSearch`. This optimizer not only finds the best examples but also searches for the best instructions and prompt formats. It requires more computational effort but can yield superior results.

## Results Overview

### üèÜ Model Performance Rankings

| Rank | Model | Accuracy | Parameters | Errors | Key Characteristics |
|------|-------|----------|------------|--------|-------------------|
| **ü•á 1st** | **Grok-4** | **99.1%** | Unknown | 4 errors | Best overall performance |
| **ü•à 2nd** | **Qwen 3-235B** | **99.0%** | 235B | 5 errors | Largest model, excellent reasoning |
| **ü•â 3rd** | **GPT-OSS-120B** | **96.1%** | 120B | 17 errors | Good balance of performance/size |
| **4th** | **GPT-OSS-20B** | **96.1%** | 20B | 17 errors | Efficient smaller model |
| **5th** | **Gemma-2B** | **14%** | 2B | 43/50 errors | Poor performance |

### openrouter/x-ai/grok-4

```
              precision    recall  f1-score   support

         Yes       1.00      0.98      0.99       197
          No       0.98      1.00      0.99       242

   micro avg       0.99      0.99      0.99       439
   macro avg       0.99      0.99      0.99       439
weighted avg       0.99      0.99      0.99       439
```

### openrouter/qwen/qwen3-235b-a22b-2507

```
              precision    recall  f1-score   support

         Yes       1.00      0.97      0.99       197
          No       0.98      1.00      0.99       242

   micro avg       0.99      0.99      0.99       439
   macro avg       0.99      0.99      0.99       439
weighted avg       0.99      0.99      0.99       439
```

### openai/gpt-oss-120b

```
              precision    recall  f1-score   support

         Yes       1.00      0.92      0.96       197
          No       0.95      0.99      0.97       242

   micro avg       0.97      0.96      0.97       439
   macro avg       0.98      0.96      0.96       439
weighted avg       0.97      0.96      0.97       439
```

### openai/gpt-oss-20b

```
              precision    recall  f1-score   support

         Yes       1.00      0.91      0.95       197
          No       0.94      0.99      0.97       242

   micro avg       0.97      0.96      0.96       439
   macro avg       0.97      0.95      0.96       439
weighted avg       0.97      0.96      0.96       439
```

### gemma:2b

```
             precision    recall  f1-score   support

         Yes       0.57      0.18      0.28        22
          No       0.00      0.00      0.00        28

   micro avg       0.57      0.08      0.14        50
   macro avg       0.29      0.09      0.14        50
weighted avg       0.25      0.08      0.12        50
```

### gpt-4.1

```
              precision    recall  f1-score   support

         Yes       1.00      0.98      0.99       197
          No       0.99      1.00      0.99       242

    accuracy                           0.99       439
   macro avg       0.99      0.99      0.99       439
weighted avg       0.99      0.99      0.99       439
```


## References
- [Abt-Buy Dataset and Benchmark](https://dbs.uni-leipzig.de/research/projects/benchmark-datasets-for-entity-resolution#:~:text=Used%20in-,Abt%2DBuy,-E%2Dcommerce)
- [DSPy: Modular LLM Pipelines](https://github.com/stanfordnlp/dspy)

---
*For research and educational use. Cite the Leipzig Database Group and VLDB2010 paper if publishing results.* 